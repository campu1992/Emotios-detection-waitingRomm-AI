{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkSs+kZDAFQT9x4sWMmGjH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/campu1992/Emotios-detection-waitingRomm-AI/blob/main/Waiting_room_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_uMQvJwbAC0"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el conjunto de datos CIFAR-10 (que contiene imágenes etiquetadas)\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocesar las imágenes\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Definir la arquitectura del modelo CNN\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(x_train, y_train, epochs=80)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "model.save('modelo_clasificacion.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhFHO3k8cMiN",
        "outputId": "aa130396-64c2-409c-bff2-c04061c73049",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4336 - accuracy: 0.4872\n",
            "Epoch 2/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0985 - accuracy: 0.6148\n",
            "Epoch 3/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9754 - accuracy: 0.6608\n",
            "Epoch 4/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8906 - accuracy: 0.6904\n",
            "Epoch 5/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8274 - accuracy: 0.7115\n",
            "Epoch 6/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7794 - accuracy: 0.7309\n",
            "Epoch 7/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7244 - accuracy: 0.7478\n",
            "Epoch 8/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6852 - accuracy: 0.7600\n",
            "Epoch 9/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6510 - accuracy: 0.7722\n",
            "Epoch 10/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6152 - accuracy: 0.7836\n",
            "Epoch 11/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5779 - accuracy: 0.7969\n",
            "Epoch 12/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5480 - accuracy: 0.8078\n",
            "Epoch 13/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5155 - accuracy: 0.8181\n",
            "Epoch 14/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4836 - accuracy: 0.8282\n",
            "Epoch 15/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4562 - accuracy: 0.8374\n",
            "Epoch 16/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4265 - accuracy: 0.8490\n",
            "Epoch 17/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4012 - accuracy: 0.8576\n",
            "Epoch 18/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3787 - accuracy: 0.8666\n",
            "Epoch 19/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3547 - accuracy: 0.8731\n",
            "Epoch 20/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3336 - accuracy: 0.8783\n",
            "Epoch 21/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3084 - accuracy: 0.8892\n",
            "Epoch 22/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2913 - accuracy: 0.8940\n",
            "Epoch 23/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2691 - accuracy: 0.9036\n",
            "Epoch 24/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2564 - accuracy: 0.9058\n",
            "Epoch 25/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2426 - accuracy: 0.9125\n",
            "Epoch 26/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2307 - accuracy: 0.9154\n",
            "Epoch 27/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2239 - accuracy: 0.9180\n",
            "Epoch 28/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2017 - accuracy: 0.9263\n",
            "Epoch 29/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1934 - accuracy: 0.9301\n",
            "Epoch 30/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1884 - accuracy: 0.9312\n",
            "Epoch 31/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1757 - accuracy: 0.9366\n",
            "Epoch 32/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1704 - accuracy: 0.9376\n",
            "Epoch 33/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1642 - accuracy: 0.9393\n",
            "Epoch 34/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1562 - accuracy: 0.9422\n",
            "Epoch 35/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1508 - accuracy: 0.9452\n",
            "Epoch 36/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1397 - accuracy: 0.9497\n",
            "Epoch 37/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1340 - accuracy: 0.9517\n",
            "Epoch 38/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1348 - accuracy: 0.9510\n",
            "Epoch 39/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1279 - accuracy: 0.9551\n",
            "Epoch 40/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1200 - accuracy: 0.9567\n",
            "Epoch 41/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1265 - accuracy: 0.9542\n",
            "Epoch 42/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1187 - accuracy: 0.9581\n",
            "Epoch 43/80\n",
            "1563/1563 [==============================] - 5s 4ms/step - loss: 0.1059 - accuracy: 0.9616\n",
            "Epoch 44/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1186 - accuracy: 0.9578\n",
            "Epoch 45/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1169 - accuracy: 0.9587\n",
            "Epoch 46/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1080 - accuracy: 0.9608\n",
            "Epoch 47/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1076 - accuracy: 0.9612\n",
            "Epoch 48/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0997 - accuracy: 0.9658\n",
            "Epoch 49/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1057 - accuracy: 0.9618\n",
            "Epoch 50/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0975 - accuracy: 0.9649\n",
            "Epoch 51/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1012 - accuracy: 0.9649\n",
            "Epoch 52/80\n",
            "1563/1563 [==============================] - 5s 4ms/step - loss: 0.0960 - accuracy: 0.9667\n",
            "Epoch 53/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0976 - accuracy: 0.9650\n",
            "Epoch 54/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0898 - accuracy: 0.9685\n",
            "Epoch 55/80\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0944 - accuracy: 0.9673\n",
            "Epoch 56/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0913 - accuracy: 0.9685\n",
            "Epoch 57/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0898 - accuracy: 0.9687\n",
            "Epoch 58/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0937 - accuracy: 0.9678\n",
            "Epoch 59/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0890 - accuracy: 0.9690\n",
            "Epoch 60/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0779 - accuracy: 0.9736\n",
            "Epoch 61/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0954 - accuracy: 0.9675\n",
            "Epoch 62/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0844 - accuracy: 0.9713\n",
            "Epoch 63/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0888 - accuracy: 0.9699\n",
            "Epoch 64/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0801 - accuracy: 0.9723\n",
            "Epoch 65/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0898 - accuracy: 0.9695\n",
            "Epoch 66/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0806 - accuracy: 0.9729\n",
            "Epoch 67/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0795 - accuracy: 0.9733\n",
            "Epoch 68/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0827 - accuracy: 0.9717\n",
            "Epoch 69/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0832 - accuracy: 0.9723\n",
            "Epoch 70/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0788 - accuracy: 0.9734\n",
            "Epoch 71/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0768 - accuracy: 0.9738\n",
            "Epoch 72/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0782 - accuracy: 0.9746\n",
            "Epoch 73/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0833 - accuracy: 0.9731\n",
            "Epoch 74/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0708 - accuracy: 0.9755\n",
            "Epoch 75/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0757 - accuracy: 0.9751\n",
            "Epoch 76/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0716 - accuracy: 0.9761\n",
            "Epoch 77/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0821 - accuracy: 0.9735\n",
            "Epoch 78/80\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0659 - accuracy: 0.9773\n",
            "Epoch 79/80\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0768 - accuracy: 0.9747\n",
            "Epoch 80/80\n",
            "1563/1563 [==============================] - 5s 4ms/step - loss: 0.0697 - accuracy: 0.9769\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 4.1406 - accuracy: 0.6581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar el modelo de clasificación de emociones:"
      ],
      "metadata": {
        "id": "lBSkob9fbFNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo de clasificación de emociones entrenado\n",
        "emotion_classifier = load_model('/content/modelo_clasificacion.h5')\n"
      ],
      "metadata": {
        "id": "7ugn0wxWbDMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Definir la función de preprocesamiento de imágenes:"
      ],
      "metadata": {
        "id": "zmUhlLX0bI3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image):\n",
        "    # Convertir la imagen a escala de grises\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Redimensionar la imagen\n",
        "    resized_image = cv2.resize(gray_image, (48, 48))\n",
        "\n",
        "    # Normalizar la imagen\n",
        "    normalized_image = resized_image / 255.0\n",
        "\n",
        "    # Expandir la dimensión de la imagen (para que sea compatible con el modelo)\n",
        "    expanded_image = np.expand_dims(normalized_image, axis=0)\n",
        "\n",
        "    return expanded_image"
      ],
      "metadata": {
        "id": "HT54wdYubIbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesar y analizar el video:"
      ],
      "metadata": {
        "id": "tNZzPX5tbQl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Abrir el video\n",
        "video_capture = cv2.VideoCapture('video.mp4')\n",
        "\n",
        "while True:\n",
        "    # Leer el siguiente fotograma del video\n",
        "    ret, frame = video_capture.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Detectar y predecir emociones en el fotograma actual\n",
        "    emotions = detect_and_predict_emotions(frame)\n",
        "\n",
        "    # Visualizar las emociones predichas en el fotograma\n",
        "    for (x, y, w, h), emotion in zip(faces, emotions):\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, emotion_labels[emotion], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # Mostrar el fotograma con las emociones predichas\n",
        "    cv2.imshow('Video Analysis', frame)\n",
        "\n",
        "    # Presionar 'q' para salir\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Cerrar la ventana y liberar recursos\n",
        "cv2.destroyAllWindows()\n",
        "video_capture.release()"
      ],
      "metadata": {
        "id": "7gD6SPIRbVD3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}